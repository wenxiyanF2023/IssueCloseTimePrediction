{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('issues.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the loaded data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['closed_at'] = pd.to_datetime(df['closed_at'])\n",
    "\n",
    "# Calculate the time to close by subtracting 'created_at' from 'closed_at'\n",
    "df['time_to_close'] = df['closed_at'] - df['created_at']\n",
    "df['time_to_close_hours'] = df['time_to_close'].dt.total_seconds() / 3600\n",
    "# drop created_at, closed_at, time_to_close\n",
    "df.drop(columns=['time_to_close'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>is_pull_request</th>\n",
       "      <th>author_association</th>\n",
       "      <th>time_to_close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[DOM] Fix package.json files for #28784</td>\n",
       "      <td>Missed some files for the react-server disallo...</td>\n",
       "      <td>2024-04-08 22:41:51+00:00</td>\n",
       "      <td>2024-04-08 22:49:19+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>COLLABORATOR</td>\n",
       "      <td>0.124444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[DOM] disallow client entrypoints with react-s...</td>\n",
       "      <td>`react-server` precludes loading code that exp...</td>\n",
       "      <td>2024-04-08 22:26:02+00:00</td>\n",
       "      <td>2024-04-08 22:37:06+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>COLLABORATOR</td>\n",
       "      <td>0.184444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[TestUtils] Build limited test-utils</td>\n",
       "      <td>We landed a flag to disable test utils in many...</td>\n",
       "      <td>2024-04-08 18:02:46+00:00</td>\n",
       "      <td>2024-04-08 19:27:20+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>COLLABORATOR</td>\n",
       "      <td>1.409444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Flight] Allow lazily resolving outlined models</td>\n",
       "      <td>We used to assume that outlined models are emi...</td>\n",
       "      <td>2024-04-08 15:24:01+00:00</td>\n",
       "      <td>2024-04-08 19:40:11+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>COLLABORATOR</td>\n",
       "      <td>4.269444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add Promise as a child test to Flight fixture</td>\n",
       "      <td>Adds a test for promise as a child that was fi...</td>\n",
       "      <td>2024-04-08 10:46:31+00:00</td>\n",
       "      <td>2024-04-08 15:06:17+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>COLLABORATOR</td>\n",
       "      <td>4.329444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0            [DOM] Fix package.json files for #28784   \n",
       "1  [DOM] disallow client entrypoints with react-s...   \n",
       "2               [TestUtils] Build limited test-utils   \n",
       "3    [Flight] Allow lazily resolving outlined models   \n",
       "4      Add Promise as a child test to Flight fixture   \n",
       "\n",
       "                                                body  \\\n",
       "0  Missed some files for the react-server disallo...   \n",
       "1  `react-server` precludes loading code that exp...   \n",
       "2  We landed a flag to disable test utils in many...   \n",
       "3  We used to assume that outlined models are emi...   \n",
       "4  Adds a test for promise as a child that was fi...   \n",
       "\n",
       "                 created_at                 closed_at  is_pull_request  \\\n",
       "0 2024-04-08 22:41:51+00:00 2024-04-08 22:49:19+00:00             True   \n",
       "1 2024-04-08 22:26:02+00:00 2024-04-08 22:37:06+00:00             True   \n",
       "2 2024-04-08 18:02:46+00:00 2024-04-08 19:27:20+00:00             True   \n",
       "3 2024-04-08 15:24:01+00:00 2024-04-08 19:40:11+00:00             True   \n",
       "4 2024-04-08 10:46:31+00:00 2024-04-08 15:06:17+00:00             True   \n",
       "\n",
       "  author_association  time_to_close_hours  \n",
       "0       COLLABORATOR             0.124444  \n",
       "1       COLLABORATOR             0.184444  \n",
       "2       COLLABORATOR             1.409444  \n",
       "3       COLLABORATOR             4.269444  \n",
       "4       COLLABORATOR             4.329444  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import contractions\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Fill NaN values with empty string before combining\n",
    "df['title'].fillna('', inplace=True)\n",
    "df['body'].fillna('', inplace=True)\n",
    "df['text'] = df['title'] + \" \" + df['body']\n",
    "\n",
    "# Step 1: Replace line breaks and quotation marks\n",
    "df['text_parsed'] = df['text'].str.replace(\"\\r\", \" \")\n",
    "df['text_parsed'] = df['text_parsed'].str.replace(\"\\n\", \" \")\n",
    "df['text_parsed'] = df['text_parsed'].str.replace('\"', '')\n",
    "df['text_parsed'] = df['text_parsed'].str.lower()\n",
    "\n",
    "# Step 2: Expand Contractions\n",
    "df['text_parsed'] = df['text_parsed'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# Step 3: Remove punctuation and possessive pronoun terminations\n",
    "punctuation_signs = string.punctuation\n",
    "df['text_parsed'] = df['text_parsed'].apply(lambda x: ''.join([char for char in x if char not in punctuation_signs]))\n",
    "df['text_parsed'] = df['text_parsed'].str.replace(\"'s\", \"\", regex=True)\n",
    "\n",
    "# Step 4: Lemmatize text\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    lemmatized_list = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text_words]\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    return lemmatized_text\n",
    "\n",
    "df['text_parsed'] = df['text_parsed'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "df['text_parsed'] = df['text_parsed'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/wenxiyang/Desktop/study/18668DS/Individual/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 26,022 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in df['text_parsed']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      \n",
    "        add_special_tokens=True,   \n",
    "        max_length=64,           \n",
    "        pad_to_max_length=True,    \n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',      \n",
    "    )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to CPU\n",
    "  batch = tuple(t for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  # Get the \"pooled\" output for each batch from BERT, which is a more \n",
    "  # fixed-sized representation for the whole sentence, and can be used for classification tasks.\n",
    "  pooled_output = outputs[1]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  predictions.append(pooled_output)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'embeddings' is a list of pooled BERT outputs\n",
    "embeddings = np.vstack(predictions)  # Stacking the list of tensors into a single numpy array\n",
    "\n",
    "# Convert BERT embeddings to a DataFrame\n",
    "bert_features = pd.DataFrame(embeddings)\n",
    "\n",
    "# Assign column names to the BERT feature columns\n",
    "bert_features.columns = [f'bert_{i}' for i in range(bert_features.shape[1])]\n",
    "\n",
    "# Concatenate the BERT embeddings DataFrame with your original DataFrame\n",
    "# Make sure the indices are aligned before concatenation\n",
    "df_bert = pd.concat([df.reset_index(drop=True), bert_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Encode the 'author_association' column with dummy variables\n",
    "author_association_dummies = pd.get_dummies(df_bert['author_association'], prefix='author')\n",
    "df_bert = pd.concat([df_bert, author_association_dummies], axis=1)\n",
    "\n",
    "# Convert 'created_at' to datetime if it's not already\n",
    "df_bert['created_at'] = pd.to_datetime(df_bert['created_at'])\n",
    "\n",
    "# Ensure the data is sorted chronologically based on 'created_at'\n",
    "df_bert = df_bert.sort_values('created_at')\n",
    "\n",
    "# Split your data chronologically into train and test sets\n",
    "# Let's say 80% for training and 20% for testing as an example\n",
    "split_point = int(len(df_bert) * 0.8)\n",
    "train_data = df_bert.iloc[:split_point, :]\n",
    "test_data = df_bert.iloc[split_point:, :]\n",
    "\n",
    "# Define the columns to drop (columns not used as features for training)\n",
    "columns_to_drop = ['created_at', 'closed_at', 'title', 'body', 'author_association', 'text', 'text_parsed']\n",
    "\n",
    "# Drop the unnecessary columns and split the data into features and target\n",
    "X_train = train_data.drop(columns=columns_to_drop + ['time_to_close_hours'], axis=1)\n",
    "y_train = train_data['time_to_close_hours']\n",
    "\n",
    "X_test = test_data.drop(columns=columns_to_drop + ['time_to_close_hours'], axis=1)\n",
    "y_test = test_data['time_to_close_hours']\n",
    "\n",
    "# Handle any NaNs in target variable 'time_to_close' if needed\n",
    "X_train = X_train[y_train.notnull()]\n",
    "y_train = y_train[y_train.notnull()]\n",
    "\n",
    "X_test = X_test[y_test.notnull()]\n",
    "y_test = y_test[y_test.notnull()]\n",
    "\n",
    "# Now, X_train, y_train, X_test, and y_test are ready for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'is_pull_request' from True/False to 0/1\n",
    "X_train['is_pull_request'] = X_train['is_pull_request'].astype(int)\n",
    "X_test['is_pull_request'] = X_test['is_pull_request'].astype(int)\n",
    "\n",
    "author_columns = ['author_COLLABORATOR', 'author_CONTRIBUTOR', 'author_MEMBER', 'author_NONE']\n",
    "\n",
    "# Convert each author_* column to numeric\n",
    "for col in author_columns:\n",
    "    X_train[col] = X_train[col].astype(int)\n",
    "    X_test[col] = X_test[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_pull_request</th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>bert_4</th>\n",
       "      <th>bert_5</th>\n",
       "      <th>bert_6</th>\n",
       "      <th>bert_7</th>\n",
       "      <th>bert_8</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_762</th>\n",
       "      <th>bert_763</th>\n",
       "      <th>bert_764</th>\n",
       "      <th>bert_765</th>\n",
       "      <th>bert_766</th>\n",
       "      <th>bert_767</th>\n",
       "      <th>author_COLLABORATOR</th>\n",
       "      <th>author_CONTRIBUTOR</th>\n",
       "      <th>author_MEMBER</th>\n",
       "      <th>author_NONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26021</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.610710</td>\n",
       "      <td>-0.262707</td>\n",
       "      <td>-0.652588</td>\n",
       "      <td>0.281311</td>\n",
       "      <td>0.325590</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.143253</td>\n",
       "      <td>-0.324933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029425</td>\n",
       "      <td>0.183692</td>\n",
       "      <td>0.456515</td>\n",
       "      <td>-0.664369</td>\n",
       "      <td>-0.498581</td>\n",
       "      <td>0.641880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26020</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.809950</td>\n",
       "      <td>-0.488104</td>\n",
       "      <td>-0.730921</td>\n",
       "      <td>0.690221</td>\n",
       "      <td>0.532908</td>\n",
       "      <td>-0.303889</td>\n",
       "      <td>0.700155</td>\n",
       "      <td>0.325999</td>\n",
       "      <td>-0.504637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.227013</td>\n",
       "      <td>0.643613</td>\n",
       "      <td>-0.576652</td>\n",
       "      <td>-0.761884</td>\n",
       "      <td>0.874527</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26019</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.839109</td>\n",
       "      <td>-0.572211</td>\n",
       "      <td>-0.866289</td>\n",
       "      <td>0.740512</td>\n",
       "      <td>0.602457</td>\n",
       "      <td>-0.244243</td>\n",
       "      <td>0.876909</td>\n",
       "      <td>0.322928</td>\n",
       "      <td>-0.505415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054007</td>\n",
       "      <td>0.182228</td>\n",
       "      <td>0.560197</td>\n",
       "      <td>-0.759059</td>\n",
       "      <td>-0.711372</td>\n",
       "      <td>0.879891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26018</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.768151</td>\n",
       "      <td>-0.280106</td>\n",
       "      <td>-0.744873</td>\n",
       "      <td>0.470827</td>\n",
       "      <td>0.520267</td>\n",
       "      <td>-0.238235</td>\n",
       "      <td>0.469454</td>\n",
       "      <td>0.115835</td>\n",
       "      <td>-0.380922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459416</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.508871</td>\n",
       "      <td>-0.821528</td>\n",
       "      <td>-0.568023</td>\n",
       "      <td>0.777994</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26017</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.848433</td>\n",
       "      <td>-0.353808</td>\n",
       "      <td>-0.355003</td>\n",
       "      <td>0.588401</td>\n",
       "      <td>0.336235</td>\n",
       "      <td>-0.249113</td>\n",
       "      <td>0.776377</td>\n",
       "      <td>0.229713</td>\n",
       "      <td>-0.148931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164339</td>\n",
       "      <td>0.444366</td>\n",
       "      <td>0.480367</td>\n",
       "      <td>-0.245089</td>\n",
       "      <td>-0.682280</td>\n",
       "      <td>0.908370</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_pull_request    bert_0    bert_1    bert_2    bert_3    bert_4  \\\n",
       "26021                1 -0.610710 -0.262707 -0.652588  0.281311  0.325590   \n",
       "26020                1 -0.809950 -0.488104 -0.730921  0.690221  0.532908   \n",
       "26019                1 -0.839109 -0.572211 -0.866289  0.740512  0.602457   \n",
       "26018                1 -0.768151 -0.280106 -0.744873  0.470827  0.520267   \n",
       "26017                1 -0.848433 -0.353808 -0.355003  0.588401  0.336235   \n",
       "\n",
       "         bert_5    bert_6    bert_7    bert_8  ...  bert_762  bert_763  \\\n",
       "26021 -0.089598  0.310309  0.143253 -0.324933  ... -0.029425  0.183692   \n",
       "26020 -0.303889  0.700155  0.325999 -0.504637  ...  0.064088  0.227013   \n",
       "26019 -0.244243  0.876909  0.322928 -0.505415  ...  0.054007  0.182228   \n",
       "26018 -0.238235  0.469454  0.115835 -0.380922  ... -0.459416  0.149000   \n",
       "26017 -0.249113  0.776377  0.229713 -0.148931  ...  0.164339  0.444366   \n",
       "\n",
       "       bert_764  bert_765  bert_766  bert_767  author_COLLABORATOR  \\\n",
       "26021  0.456515 -0.664369 -0.498581  0.641880                    0   \n",
       "26020  0.643613 -0.576652 -0.761884  0.874527                    0   \n",
       "26019  0.560197 -0.759059 -0.711372  0.879891                    0   \n",
       "26018  0.508871 -0.821528 -0.568023  0.777994                    0   \n",
       "26017  0.480367 -0.245089 -0.682280  0.908370                    0   \n",
       "\n",
       "       author_CONTRIBUTOR  author_MEMBER  author_NONE  \n",
       "26021                   1              0            0  \n",
       "26020                   1              0            0  \n",
       "26019                   1              0            0  \n",
       "26018                   1              0            0  \n",
       "26017                   1              0            0  \n",
       "\n",
       "[5 rows x 773 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training RMSE: 4024.56\n",
      "Random Forest Test RMSE: 1637.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_reg.fit(X_train, y_train_log)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "train_predictions_rf = np.expm1(rf_reg.predict(X_train))\n",
    "test_predictions_rf = np.expm1(rf_reg.predict(X_test))\n",
    "\n",
    "# Calculate RMSE for both the training and testing sets\n",
    "train_rmse_rf = sqrt(mean_squared_error(y_train, train_predictions_rf))\n",
    "test_rmse_rf = sqrt(mean_squared_error(y_test, test_predictions_rf))\n",
    "\n",
    "print(f\"Random Forest Training RMSE: {train_rmse_rf:.2f}\")\n",
    "print(f\"Random Forest Test RMSE: {test_rmse_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "model_path = \"random_forest_model.joblib\"\n",
    "dump(rf_reg, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
